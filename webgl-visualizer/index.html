<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <style>
      body {
        margin: 0;
        overflow: hidden;
        background: #000;
        color: #fff;
        font-family: sans-serif;
      }
      canvas {
        display: block;
      }
      #controls {
        position: fixed;
        top: 10px;
        left: 10px;
        background: rgba(0, 0, 0, 0.6);
        padding: 10px;
        border-radius: 5px;
        z-index: 100;
      }
      #modeDisplay {
        margin-bottom: 10px;
        font-weight: bold;
      }
      button,
      input {
        margin: 5px;
      }
    </style>
  </head>
  <body>
    <canvas id="canvas"></canvas>
    <div id="controls">
      <div id="modeDisplay">Mode: Loading...</div>
      <input type="file" id="audioUpload" accept="audio/*" />
      <button id="startRecording">Start Recording</button>
      <button id="stopRecording" disabled>Stop Recording</button>
      <div style="margin-top: 10px;">Use ← and → keys to switch animations</div>
    </div>
    <script>
      const canvas = document.getElementById("canvas");
      const gl = canvas.getContext("webgl", {
        antialias: true,
        preserveDrawingBuffer: true,
      });
      if (!gl) {
        alert("WebGL not supported");
        throw new Error("WebGL not supported");
      }
      function resizeCanvas() {
        const dpr = window.devicePixelRatio || 1;
        canvas.width = window.innerWidth * dpr;
        canvas.height = window.innerHeight * dpr;
        canvas.style.width = window.innerWidth + "px";
        canvas.style.height = window.innerHeight + "px";
        gl.viewport(0, 0, canvas.width, canvas.height);
      }
      window.addEventListener("resize", resizeCanvas);
      resizeCanvas();
      const vsSource = `
        attribute vec2 aPosition;
        void main(){
          gl_Position = vec4(aPosition, 0.0, 1.0);
        }
      `;
      const fsSource1 = `
        precision mediump float;
        uniform float uTime;
        uniform vec2 uResolution;
        uniform float uAudioLevel;
        
        float hash(vec2 p){
          return fract(sin(dot(p, vec2(127.1,311.7))) * 43758.5453123);
        }
        float noise(vec2 p){
          vec2 i = floor(p);
          vec2 f = fract(p);
          float a = hash(i);
          float b = hash(i + vec2(1.0, 0.0));
          float c = hash(i + vec2(0.0, 1.0));
          float d = hash(i + vec2(1.0, 1.0));
          vec2 u = f*f*(3.0-2.0*f);
          return mix(a, b, u.x) + (c - a)*u.y*(1.0-u.x) + (d - b)*u.x*u.y;
        }
        float fbm(vec2 p){
          float total = 0.0;
          float amplitude = 0.5;
          for(int i=0; i<6; i++){
            total += noise(p) * amplitude;
            p *= 2.0;
            amplitude *= 0.5;
          }
          return total;
        }
        void main(void){
          vec2 uv = gl_FragCoord.xy / uResolution.xy;
          vec2 p = uv - 0.5;
          p.x *= uResolution.x / uResolution.y;
          float angle = uTime * 0.3 + uAudioLevel * 3.0;
          float s = sin(angle);
          float c = cos(angle);
          mat2 rot = mat2(c, -s, s, c);
          p = rot * p;
          float n = fbm(p * 3.0 + uTime * 0.5);
          vec3 colorA = vec3(0.1, 0.2, 0.5);
          vec3 colorB = vec3(1.0, 0.5, 0.2);
          vec3 col = mix(colorA, colorB, n);
          col += uAudioLevel * 0.5;
          gl_FragColor = vec4(pow(col, vec3(0.4545)), 1.0);
        }
      `;
      const fsSource2 = `
        precision mediump float;
        uniform float uTime;
        uniform vec2 uResolution;
        uniform float uAudioLevel;
        void main(void) {
          vec2 uv = (gl_FragCoord.xy - 0.5 * uResolution) / uResolution.y;
          float angle = atan(uv.y, uv.x);
          float radius = length(uv);
          float wave = sin(10.0 * radius - uTime * 2.0 + uAudioLevel * 5.0) + sin(5.0 * angle + uTime);
          vec3 col = vec3(
            0.5 + 0.5 * cos(uTime + wave + 0.0),
            0.5 + 0.5 * cos(uTime + wave + 2.0),
            0.5 + 0.5 * cos(uTime + wave + 4.0)
          );
          gl_FragColor = vec4(col, 1.0);
        }
      `;
      const fsSource3 = `
        precision mediump float;
        uniform float uTime;
        uniform vec2 uResolution;
        uniform float uAudioLevel;
        void main(void) {
          vec2 uv = (gl_FragCoord.xy - 0.5 * uResolution) / uResolution.y;
          float angle = atan(uv.y, uv.x);
          float radius = length(uv);
          float stripes = sin(10.0 * radius - uTime * 5.0 + uAudioLevel * 10.0);
          vec3 col = vec3(
            0.5 + 0.5 * cos(uTime + stripes + angle),
            0.5 + 0.5 * sin(uTime + stripes + angle),
            0.5 + 0.5 * cos(uTime + stripes)
          );
          gl_FragColor = vec4(col, 1.0);
        }
      `;
      function compileShader(source, type) {
        const shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
          const error = gl.getShaderInfoLog(shader);
          gl.deleteShader(shader);
          throw new Error("Shader compile error: " + error);
        }
        return shader;
      }
      function createProgram(fsSource) {
        const vertexShader = compileShader(vsSource, gl.VERTEX_SHADER);
        const fragmentShader = compileShader(fsSource, gl.FRAGMENT_SHADER);
        const program = gl.createProgram();
        gl.attachShader(program, vertexShader);
        gl.attachShader(program, fragmentShader);
        gl.linkProgram(program);
        if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
          const error = gl.getProgramInfoLog(program);
          gl.deleteProgram(program);
          throw new Error("Program link error: " + error);
        }
        return program;
      }
      const animations = [];
      const fsSources = [fsSource1, fsSource2, fsSource3];
      const modeNames = ["Noise", "Waves", "Tunnel"];
      for (let i = 0; i < fsSources.length; i++) {
        const program = createProgram(fsSources[i]);
        gl.useProgram(program);
        const uTime = gl.getUniformLocation(program, "uTime");
        const uResolution = gl.getUniformLocation(program, "uResolution");
        const uAudioLevel = gl.getUniformLocation(program, "uAudioLevel");
        animations.push({
          name: modeNames[i],
          program,
          uniforms: { uTime, uResolution, uAudioLevel },
        });
      }
      let currentAnimationIndex = 0;
      const modeDisplay = document.getElementById("modeDisplay");
      function updateModeDisplay() {
        modeDisplay.textContent = "Mode: " + animations[currentAnimationIndex].name;
      }
      updateModeDisplay();
      const vertices = new Float32Array([
        -1, -1,   1, -1,   -1, 1,
        -1,  1,   1, -1,    1, 1
      ]);
      const buffer = gl.createBuffer();
      gl.bindBuffer(gl.ARRAY_BUFFER, buffer);
      gl.bufferData(gl.ARRAY_BUFFER, vertices, gl.STATIC_DRAW);
      function bindAttributes(program) {
        const aPosition = gl.getAttribLocation(program, "aPosition");
        gl.enableVertexAttribArray(aPosition);
        gl.vertexAttribPointer(aPosition, 2, gl.FLOAT, false, 0, 0);
      }
      window.addEventListener("keydown", (e) => {
        if (e.key === "ArrowRight") {
          currentAnimationIndex = (currentAnimationIndex + 1) % animations.length;
          updateModeDisplay();
        } else if (e.key === "ArrowLeft") {
          currentAnimationIndex = (currentAnimationIndex - 1 + animations.length) % animations.length;
          updateModeDisplay();
        }
      });
      let audioBuffer = null;
      let analyser = null;
      let dataArray = null;
      let sourceNode = null;
      const AudioContext = window.AudioContext || window.webkitAudioContext;
      const audioCtx = new AudioContext();
      const dest = audioCtx.createMediaStreamDestination();
      document.getElementById("audioUpload").addEventListener("change", async (e) => {
        const file = e.target.files[0];
        if (file) {
          const arrayBuffer = await file.arrayBuffer();
          audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
          console.log("Audio file loaded.");
        }
      });
      let startTime = null;
      function render(timestamp) {
        if (!startTime) startTime = timestamp;
        const elapsed = (timestamp - startTime) / 1000.0;
        const anim = animations[currentAnimationIndex];
        gl.useProgram(anim.program);
        bindAttributes(anim.program);
        gl.clearColor(0.0, 0.0, 0.0, 1.0);
        gl.clear(gl.COLOR_BUFFER_BIT);
        gl.uniform1f(anim.uniforms.uTime, elapsed);
        gl.uniform2f(anim.uniforms.uResolution, canvas.width, canvas.height);
        let audioLevel = 0.0;
        if (analyser && dataArray) {
          analyser.getByteFrequencyData(dataArray);
          let sum = 0;
          for (let i = 0; i < dataArray.length; i++) {
            sum += dataArray[i];
          }
          audioLevel = sum / dataArray.length / 255;
        }
        gl.uniform1f(anim.uniforms.uAudioLevel, audioLevel);
        gl.drawArrays(gl.TRIANGLES, 0, 6);
        requestAnimationFrame(render);
      }
      requestAnimationFrame(render);
      let mediaRecorder;
      let chunks = [];
      const videoBitrate = 10000000;
      document.getElementById("startRecording").addEventListener("click", async () => {
        await audioCtx.resume();
        chunks = [];
        if (audioBuffer) {
          if (sourceNode) {
            try { sourceNode.stop(); } catch (e) {}
          }
          sourceNode = audioCtx.createBufferSource();
          sourceNode.buffer = audioBuffer;
          const gainNode = audioCtx.createGain();
          sourceNode.connect(gainNode);
          analyser = audioCtx.createAnalyser();
          analyser.fftSize = 512;
          dataArray = new Uint8Array(analyser.frequencyBinCount);
          gainNode.connect(analyser);
          gainNode.connect(audioCtx.destination);
          gainNode.connect(dest);
          sourceNode.start(0);
        }
        const canvasStream = canvas.captureStream(60);
        const audioStream = dest.stream;
        const tracks = [
          ...canvasStream.getTracks(),
          ...audioStream.getTracks()
        ];
        const combinedStream = new MediaStream(tracks);
        mediaRecorder = new MediaRecorder(combinedStream, {
          mimeType: "video/webm;codecs=vp9,opus",
          videoBitsPerSecond: videoBitrate,
        });
        mediaRecorder.ondataavailable = (e) => {
          if (e.data && e.data.size > 0) chunks.push(e.data);
        };
        mediaRecorder.onstop = () => {
          const blob = new Blob(chunks, { type: "video/webm" });
          const url = URL.createObjectURL(blob);
          const a = document.createElement("a");
          a.href = url;
          a.download = "visualization.webm";
          document.body.appendChild(a);
          a.click();
          URL.revokeObjectURL(url);
          a.remove();
        };
        mediaRecorder.start(1000);
        document.getElementById("startRecording").disabled = true;
        document.getElementById("stopRecording").disabled = false;
      });
      
      document.getElementById("stopRecording").addEventListener("click", () => {
        mediaRecorder.stop();
        document.getElementById("startRecording").disabled = false;
        document.getElementById("stopRecording").disabled = true;
        if (sourceNode) {
          try { sourceNode.stop(); } catch (e) {}
        }
      });
    </script>
  </body>
</html>
