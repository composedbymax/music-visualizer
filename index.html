<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <style>
      body {
        margin: 0;
        overflow: hidden;
      }
      canvas {
        display: block;
        width: 100vw;
        height: 100vh;
      }
      #controls {
        position: fixed;
        top: 10px;
        left: 10px;
        z-index: 100;
        background: rgba(255, 255, 255, 0.8);
        padding: 10px;
        border-radius: 4px;
      }
      button,
      input {
        margin-right: 5px;
      }
    </style>
  </head>
  <body>
    <canvas id="canvas"></canvas>
    <div id="controls">
      <input type="file" id="audioUpload" accept="audio/*" />
      <button id="startRecording">Start Recording</button>
      <button id="stopRecording" disabled>Stop Recording</button>
    </div>
    <script>
      const canvas = document.getElementById("canvas");
      const gl = canvas.getContext("webgl");
      if (!gl) {
        alert("WebGL not supported");
        throw new Error("WebGL not supported");
      }
      function resizeCanvas() {
        canvas.width = window.innerWidth;
        canvas.height = window.innerHeight;
        gl.viewport(0, 0, gl.drawingBufferWidth, gl.drawingBufferHeight);
      }
      window.addEventListener("resize", resizeCanvas);
      resizeCanvas();
      const vsSource = ` attribute vec2 aPosition;void main(){gl_Position=vec4(aPosition,0.0,1.0);}`;
      const fsSource = ` precision mediump float;uniform float uTime;uniform vec2 uResolution;uniform float uAudioLevel;void main(void){vec2 uv=gl_FragCoord.xy/uResolution.xy;vec3 color1=0.5+0.5*cos(uTime+uv.xyx+vec3(0.0,2.0,4.0));float dist=distance(uv,vec2(0.5));vec3 color2=mix(vec3(1.0,0.2,0.3),vec3(0.2,0.3,1.0),smoothstep(0.3,0.6,dist));float blend=0.5+0.5*sin(uTime*0.5);blend*=(0.5+uAudioLevel);vec3 finalColor=mix(color1,color2,blend);gl_FragColor=vec4(finalColor,1.0);}`;
      function compileShader(source, type) {
        const shader = gl.createShader(type);
        gl.shaderSource(shader, source);
        gl.compileShader(shader);
        if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {
          const error = gl.getShaderInfoLog(shader);
          gl.deleteShader(shader);
          throw new Error("Shader compile error: " + error);
        }
        return shader;
      }
      const vertexShader = compileShader(vsSource, gl.VERTEX_SHADER);
      const fragmentShader = compileShader(fsSource, gl.FRAGMENT_SHADER);
      const program = gl.createProgram();
      gl.attachShader(program, vertexShader);
      gl.attachShader(program, fragmentShader);
      gl.linkProgram(program);
      if (!gl.getProgramParameter(program, gl.LINK_STATUS)) {
        const error = gl.getProgramInfoLog(program);
        gl.deleteProgram(program);
        throw new Error("Program link error: " + error);
      }
      gl.useProgram(program);
      const vertices = new Float32Array([
        -1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1,
      ]);
      const buffer = gl.createBuffer();
      gl.bindBuffer(gl.ARRAY_BUFFER, buffer);
      gl.bufferData(gl.ARRAY_BUFFER, vertices, gl.STATIC_DRAW);
      const aPosition = gl.getAttribLocation(program, "aPosition");
      gl.enableVertexAttribArray(aPosition);
      gl.vertexAttribPointer(aPosition, 2, gl.FLOAT, false, 0, 0);
      const uTimeLocation = gl.getUniformLocation(program, "uTime");
      const uResolutionLocation = gl.getUniformLocation(program, "uResolution");
      const uAudioLevelLocation = gl.getUniformLocation(program, "uAudioLevel");
      let audioBuffer = null;
      let analyser = null;
      let dataArray = null;
      let sourceNode = null;
      const AudioContext = window.AudioContext || window.webkitAudioContext;
      const audioCtx = new AudioContext();
      const dest = audioCtx.createMediaStreamDestination();
      document
        .getElementById("audioUpload")
        .addEventListener("change", async (e) => {
          const file = e.target.files[0];
          if (file) {
            const arrayBuffer = await file.arrayBuffer();
            audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
            console.log("Audio file loaded.");
          }
        });
      let startTime = null;
      function render(timestamp) {
        if (!startTime) startTime = timestamp;
        const elapsed = (timestamp - startTime) / 1000.0;
        gl.clearColor(0.0, 0.0, 0.0, 1.0);
        gl.clear(gl.COLOR_BUFFER_BIT);
        gl.uniform1f(uTimeLocation, elapsed);
        gl.uniform2f(uResolutionLocation, canvas.width, canvas.height);
        let audioLevel = 0.0;
        if (analyser && dataArray) {
          analyser.getByteFrequencyData(dataArray);
          let sum = 0;
          for (let i = 0; i < dataArray.length; i++) {
            sum += dataArray[i];
          }
          audioLevel = sum / dataArray.length / 255;
        }
        gl.uniform1f(uAudioLevelLocation, audioLevel);
        gl.drawArrays(gl.TRIANGLES, 0, 6);
        requestAnimationFrame(render);
      }
      requestAnimationFrame(render);
      let mediaRecorder;
      let chunks = [];
      const videoBitrate = 2500000;
      document
        .getElementById("startRecording")
        .addEventListener("click", async () => {
          await audioCtx.resume();
          chunks = [];
          if (audioBuffer) {
            if (sourceNode) {
              try {
                sourceNode.stop();
              } catch (e) {}
            }
            sourceNode = audioCtx.createBufferSource();
            sourceNode.buffer = audioBuffer;
            const gainNode = audioCtx.createGain();
            sourceNode.connect(gainNode);
            analyser = audioCtx.createAnalyser();
            analyser.fftSize = 256;
            dataArray = new Uint8Array(analyser.frequencyBinCount);
            gainNode.connect(analyser);
            gainNode.connect(audioCtx.destination);
            gainNode.connect(dest);
            sourceNode.start(0);
          }
          const canvasStream = canvas.captureStream(60);
          const audioStream = dest.stream;
          const tracks = [
            ...canvasStream.getTracks(),
            ...audioStream.getTracks(),
          ];
          const combinedStream = new MediaStream(tracks);
          mediaRecorder = new MediaRecorder(combinedStream, {
            mimeType: "video/webm;codecs=vp8,opus",
            videoBitsPerSecond: videoBitrate,
          });
          mediaRecorder.ondataavailable = (e) => {
            if (e.data && e.data.size > 0) {
              chunks.push(e.data);
            }
          };
          mediaRecorder.onstop = () => {
            const blob = new Blob(chunks, { type: "video/webm" });
            const url = URL.createObjectURL(blob);
            const a = document.createElement("a");
            a.href = url;
            a.download = "visualization.webm";
            document.body.appendChild(a);
            a.click();
            URL.revokeObjectURL(url);
            a.remove();
          };
          mediaRecorder.start(1000);
          document.getElementById("startRecording").disabled = true;
          document.getElementById("stopRecording").disabled = false;
        });
      document.getElementById("stopRecording").addEventListener("click", () => {
        mediaRecorder.stop();
        document.getElementById("startRecording").disabled = false;
        document.getElementById("stopRecording").disabled = true;
        if (sourceNode) {
          try {
            sourceNode.stop();
          } catch (e) {}
        }
      });
    </script>
  </body>
</html>